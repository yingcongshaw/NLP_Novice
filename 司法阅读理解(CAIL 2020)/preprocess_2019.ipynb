{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "import gzip\n",
    "import pickle\n",
    "from baseline.data_process import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取2019年的原训练数据集\n",
    "with open('./data2019/big_train_data.json', 'r', encoding='utf-8') as reader:\n",
    "    full_data_2019 = json.load(reader) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_context_to_2020(context_2019):\n",
    "    '''\n",
    "    把2019年案例内容根据标点符号分句，构造成2020年的原始标准格式\n",
    "    '''\n",
    "    context_2020=[]\n",
    "    pattern = r',|\\.|\\:|;|!|\\?|:|，|。|：|；|！|？'\n",
    "    result_list = re.split(pattern, context_2019)\n",
    "    context_2020.append(result_list[0])\n",
    "    context_2020.append(result_list)\n",
    "    return [context_2020]\n",
    "\n",
    "def get_supporting_facts_sen_id(context_2019, answer_start, answer_text):\n",
    "    \n",
    "    '''\n",
    "    把2019年案例的回答依据，构造成2020年的格式。\n",
    "    '''\n",
    "    pattern = r',|\\.|\\:|;|!|\\?|:|，|。|：|；|！|？'\n",
    "    sen_list = re.split(pattern, context_2019)\n",
    "    answer_index = 0\n",
    "    sen_id = {}\n",
    "    for i, sen in enumerate(sen_list):\n",
    "        # 在分句后的案例内容中查找行号\n",
    "        index = sen.find(answer_text)\n",
    "        if index >=0 :\n",
    "            sen_id[i] = abs(answer_start - answer_index - index)\n",
    "        answer_index += len(sen)\n",
    "    supporting_fact = []\n",
    "    supporting_fact.append(sen_list[0])\n",
    "    if len(sen_id) ==0 :\n",
    "        supporting_fact.append(-1)\n",
    "    else:\n",
    "        supporting_fact.append( min(sen_id, key=sen_id.get))\n",
    "    return [supporting_fact]\n",
    "\n",
    "train_2019 = []\n",
    "for i, case_2019 in enumerate(full_data_2019['data']):\n",
    "    case_2020= {}\n",
    "    # 遍历qas, \n",
    "    question_2019 = {}\n",
    "    # 先取question_2019为true\n",
    "    for qa in case_2019['paragraphs'][0]['qas']:\n",
    "        # 过滤answers为空的question\n",
    "        if qa['is_impossible']=='true' and  len(qa['answers']) > 0 and qa['answers'][0]['answer_start'] !=-1:\n",
    "            question_2019=qa\n",
    "            break\n",
    "    if len(question_2019)==0:       \n",
    "        for qa in case_2019['paragraphs'][0]['qas']:\n",
    "            # 过滤answers为空的question\n",
    "            if qa['is_impossible']=='false' and len(qa['answers']) > 0 and qa['answers'][0]['answer_start'] !=-1:\n",
    "                    question_2019=qa\n",
    "                    answer_txt =qa['answers'][0]['text']\n",
    "                    # 取出answers的text为yes或no的第一个question\n",
    "                    if answer_txt.lower() == 'yes' or answer_txt.lower() == 'no':\n",
    "                        break\n",
    "    case_2020['_id'] = i+5055\n",
    "    case_2020['context'] = convert_context_to_2020(case_2019['paragraphs'][0]['context'])\n",
    "    case_2020['question'] = question_2019['question']\n",
    "    case_2020['answer'] = question_2019['answers'][0]['text']\n",
    "    case_2020['supporting_facts'] =get_supporting_facts_sen_id(case_2019['paragraphs'][0]['context'],\n",
    "                                                           question_2019['answers'][0]['answer_start'],\n",
    "                                                           question_2019['answers'][0]['text'])\n",
    "    \n",
    "    train_2019.append(case_2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把处理好的2019年数据集，保存到data文件夹下\n",
    "with open('./data/train_2019.json', 'w', encoding='utf-8') as writer:\n",
    "    data=json.dumps(train_2019,ensure_ascii=False)\n",
    "    writer.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把处理好的2019年数据集，合并2020年数据集，输出到新文件中\n",
    "# with open('./data/train_2019_to_2020.json', 'w', encoding='utf-8') as writer:\n",
    "#     with open('./data/train.json', 'r', encoding='utf-8') as reader:\n",
    "#         train_2020=json.load(reader)\n",
    "#         train_2020.extend(train_2019)\n",
    "#         data=json.dumps(train_2020,ensure_ascii=False)\n",
    "#         writer.write(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
